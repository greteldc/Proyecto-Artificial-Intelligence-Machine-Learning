{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS PREDICTIVO DE UN INGRESO DE MÁS DE 50.000 DOLARES ANUALES\n",
    "\n",
    "## 1. Propósito del dataset y su uso para una aplicación de machine learning\n",
    "\n",
    "Tenemos a disposición un dataset de personas con un ingreso de más o menos de 50.000 dólares anuales. Las personas son presentadas con varios rasgos (ej. sexo, situación familiar, nivel de educación). Examinaremos en qué medida estas variables permiten predecir si una persona tiene un ingreso de más de 50.000 dólares. Investigaremos también qué modelo permite predecir mejor el número real de personas con un ingreso de más de 50.000 dólares.  \n",
    "\n",
    "Nos planteamos el contexto siguiente: las personas con un ingreso de más de 50.000 pueden pertenecer a la categoría que más interesada pueda estar en una oferta de inversión que tiene nuestra organización. No disponemos de información \"directa\" sobre el sueldo de un conjunto de personas que tenemos en nuestros registros, pero sí disponemos de ciertas características de cada una de estas personas, que corresponden a las características de unos registros que sí dispone de esta información. Dado que nos interesa antes que nada encontrar la mayor parte posible de todas aquellas personas con un ingreso de 50.000, y que no nos importa tanto que se haya incluido alguna persona con un ingreso menor, la **métrica** que vamos a adoptar para evaluar el buen funcionamiento de nuestro modelos para la predicción, es el **recall o la sensibilidad**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Desarrollo del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset usado: al indagar un poco más en los datos que íbamos a usar, la fuente donde primero habíamos encontrado el dataset presentaba solo una parte de los datos. Descargamos los datos pues, de la fuente original (UCI), y juntamos los datos de train y test que ya iban separados (véase también el notebook de DATOS). Así aplicamos un nuevo split train/test sobre el conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://archive.ics.uci.edu/ml/datasets/adult (fuente usada con los datos originales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el notebook hemos dejado documentado el desarrollo del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. \"Lessons learnt\" y problemas encontrados (parcialmente solucionados)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para una próxima ocasión:\n",
    "- aplicar enseguida un 'strip' de todos los valores\n",
    "- trabajar antes con funciones en lugar de primeras versiones más manuales aún (por falta de tiempo ha habido menos conversión a funciones de lo querido)\n",
    "- no \"-\" en .py si no es para números negativos\n",
    "- código de nivel aún bajo (ej. poca encapsulación), por mejorar aún."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mayores problemas con los cuales nos hemos encontrado a lo largo del desarrollo del proyecto:\n",
    "\n",
    "- experimenté el análisis de datos, en particular el decidir qué datos incluir y cuáles no en el entrenamiento del modelo, como una parte bastante difícil\n",
    "- imposibilidad de leer datos en .py: después de muchas odiseas, justo antes de entregar parece que encontré una solución. Hasta entonces me vi obligada a trabajar en local con ruta absoluta. Si no os funcionara la lectura de los datos/escritura del modelo, estaría agradecida saberlo (que entonces no solucioné nada y cabría seguir con el tema...)\n",
    "- cambiar variables a datatype a int32 puede tener consecuencias para el procesado de ML posterior?\n",
    "- OneHotEncoder dando output \"NaN\". Al final origen simple: el haber quitado filas sin haber vuelto a poner el índice correspondiente a cada fila. Al principio lo habíamos dejado de propósito, para no perder el 'origen' de cada fila indicada al principio (dataset 1 o 2), pero probablemente será mejor siempre adaptar los índices correspondientes al hacer adiciones/eliminaciones de filas?\n",
    "- ¿encontrar el buen balance entre Encoding (si output en varias columnas)  vs.   colinearidad (de lo que entendimos, lo primero provacaría 'automáticamente' lo segundo)?\n",
    "- un mejor conocimiento del tema habría permitido tener pistas más motivadas de selección de modelos + de selección de los parámetros. \n",
    "- a pesar de una revisión posible de la materia sobre los modelos de supervised learning + pipelines, me salieron aún muchos errores en la aplicación de pipelines. Gracias a la ayuda de algún compañero de clase, creo que al final he podido evitar muchos de ellos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
